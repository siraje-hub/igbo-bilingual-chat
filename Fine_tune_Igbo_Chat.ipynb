{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nwokike/igbo-bilingual-chat/blob/main/Fine_tune_Igbo_Chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cell 1: Setup\n",
        "# --- 1. Install Libraries ---\n",
        "print(\"--- [1/10] Installing libraries (Unsloth, Transformers, etc.)... ---\")\n",
        "!pip install \"unsloth[colab-new]\" transformers peft bitsandbytes datasets trl\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "import time\n",
        "import os\n",
        "from huggingface_hub import login, create_repo, HfApi, snapshot_download\n",
        "import random\n",
        "import getpass\n",
        "print(\"--- ‚úÖ [1/10] Libraries installed ---\")\n",
        "\n",
        "# --- 2. HUGGING FACE LOGIN & AUTO-REPO CREATION ---\n",
        "print(\"\\n--- [2/10] Hugging Face Login & Repo Setup ---\")\n",
        "global token\n",
        "token = getpass.getpass(\"Please paste your Hugging Face 'write' token: \")\n",
        "login(token=token)\n",
        "\n",
        "try:\n",
        "    whoami_info = HfApi().whoami(token=token)\n",
        "    whoami = whoami_info['name']\n",
        "    print(f\"--- Logged in as: {whoami} ---\")\n",
        "except Exception as e:\n",
        "    print(f\"--- ‚ö†Ô∏è Could not get username. Please check your token. Error: {e} ---\")\n",
        "    raise e\n",
        "\n",
        "global NEW_HUB_REPO\n",
        "NEW_HUB_REPO = f\"{whoami}/Igbo-Phi3-Bilingual-Chat-v1\"\n",
        "print(f\"--- Your new checkpoints will be saved to: {NEW_HUB_REPO} ---\")\n",
        "\n",
        "print(f\"--- Checking for/creating repo {NEW_HUB_REPO}...\")\n",
        "try:\n",
        "    create_repo(repo_id=NEW_HUB_REPO, exist_ok=True, repo_type=\"model\")\n",
        "    print(f\"--- ‚úÖ [2/10] Repo '{NEW_HUB_REPO}' is ready! ---\")\n",
        "except Exception as e:\n",
        "    print(f\"--- ‚ö†Ô∏è Could not create repo. Error: {e} ---\")\n",
        "    raise e\n",
        "\n",
        "# --- 3. Load the *BASE* Model & Tokenizer ---\n",
        "print(\"\\n--- [3/10] Loading base Phi-3 model... ---\")\n",
        "max_seq_length = 1024\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "global model, tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/phi-3-mini-4k-instruct-bnb-4bit\", # Correct lowercase\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "print(\"--- ‚úÖ [3/10] Base model loaded ---\")\n",
        "\n",
        "# --- 4. Add PEFT/LoRA Adapters ---\n",
        "print(\"\\n--- [4/10] Adding PEFT/LoRA adapters... ---\")\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = True,\n",
        "    random_state = 42,\n",
        "    max_seq_length = max_seq_length,\n",
        ")\n",
        "print(\"--- ‚úÖ [4/10] PEFT adapters added ---\")\n",
        "\n",
        "# --- 5. Load Dataset 1: \"Fluency\" (522k Sentences) ---\n",
        "print(\"\\n--- [5/10] Loading 1/3: Igbo Sentences (ccibeekeoc42)... ---\")\n",
        "igbo_sentences = load_dataset(\"ccibeekeoc42/english_to_igbo\", split=\"train\")\n",
        "\n",
        "def format_translation_chat(example):\n",
        "    if not (example.get('English') and example.get('Igbo')):\n",
        "        return {\"text\": None}\n",
        "    eng = example['English']\n",
        "    igb = example['Igbo']\n",
        "    templates = [\n",
        "        f\"<s><|user|>\\nHow would I say '{eng}' in Igbo?<|end|>\\n<|assistant|>\\nYou would say '{igb}'.<|end|>\",\n",
        "        f\"<s><|user|>\\nWhat's the Igbo for '{eng}'?<|end|>\\n<|assistant|>\\n{igb}<|end|>\",\n",
        "        f\"<s><|user|>\\n{igb}<|end|>\\n<|assistant|>\\n{eng}<|end|>\"\n",
        "    ]\n",
        "    return {\"text\": random.choice(templates)}\n",
        "\n",
        "formatted_igbo_chat = igbo_sentences.map(\n",
        "    format_translation_chat,\n",
        "    remove_columns=list(igbo_sentences.features),\n",
        "    num_proc=os.cpu_count() # <-- FIX: Use all CPU cores\n",
        ").filter(lambda x: x[\"text\"] is not None)\n",
        "print(f\"--- ‚úÖ [5/10] Loaded {len(formatted_igbo_chat)} 'Fluency' examples ---\")\n",
        "\n",
        "# --- 6. Load Dataset 2: \"Vocabulary\" (11.7MB Dictionary) ---\n",
        "print(\"\\n--- [6/10] Loading 2/3: Igbo Dictionary (nkowaokwu/ibo-dict)... ---\")\n",
        "igbo_dict = load_dataset(\"nkowaokwu/ibo-dict\", data_files=\"ibo-dict.json\", split=\"train\")\n",
        "\n",
        "def format_dictionary_chat(example):\n",
        "    if not (example.get('igbo') and example.get('english')):\n",
        "        return {\"text\": None}\n",
        "    try:\n",
        "        igb_word = example['igbo']\n",
        "        eng_def = example['english']\n",
        "    except (IndexError, TypeError):\n",
        "        return {\"text\": None}\n",
        "    if not (igb_word and eng_def):\n",
        "        return {\"text\": None}\n",
        "\n",
        "    templates = [\n",
        "        f\"<s><|user|>\\nWhat does the Igbo word '{igb_word}' mean?<|end|>\\n<|assistant|>\\nIt means: {eng_def}<|end|>\",\n",
        "        f\"<s><|user|>\\nDefine '{igb_word}'.<|end|>\\n<|assistant|>\\n{igb_word} means {eng_def}.<|end|>\"\n",
        "    ]\n",
        "    return {\"text\": random.choice(templates)}\n",
        "\n",
        "formatted_igbo_dict = igbo_dict.map(\n",
        "    format_dictionary_chat,\n",
        "    remove_columns=list(igbo_dict.features),\n",
        "    num_proc=os.cpu_count() # <-- FIX: Use all CPU cores\n",
        ").filter(lambda x: x[\"text\"] is not None)\n",
        "print(f\"--- ‚úÖ [6/10] Loaded {len(formatted_igbo_dict)} 'Vocabulary' examples ---\")\n",
        "\n",
        "# --- 7. Load Dataset 3: \"Memory\" (200k Chat) ---\n",
        "print(\"\\n--- [7/10] Loading 3/3: General Chat (HuggingFaceH4/ultrachat_200k)... ---\")\n",
        "general_chat = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft\")\n",
        "\n",
        "def format_general_chat(example):\n",
        "    try:\n",
        "        if len(example['messages']) >= 2 and \\\n",
        "           example['messages'][0]['role'] == 'user' and \\\n",
        "           example['messages'][1]['role'] == 'assistant':\n",
        "            user_prompt = example['messages'][0]['content']\n",
        "            ai_response = example['messages'][1]['content']\n",
        "            if \"translate\" in user_prompt.lower():\n",
        "                return {\"text\": None}\n",
        "            text = f\"<s><|user|>\\n{user_prompt}<|end|>\\n<|assistant|>\\n{ai_response}<|end|>\"\n",
        "            return {\"text\": text}\n",
        "    except Exception:\n",
        "        pass\n",
        "    return {\"text\": None}\n",
        "\n",
        "formatted_general_chat = general_chat.map(\n",
        "    format_general_chat,\n",
        "    remove_columns=list(general_chat.features),\n",
        "    num_proc=os.cpu_count() # <-- FIX: Use all CPU cores\n",
        ").filter(lambda x: x[\"text\"] is not None)\n",
        "print(f\"--- ‚úÖ [7/10] Loaded {len(formatted_general_chat)} 'Memory' examples ---\")\n",
        "\n",
        "# --- 8. Combine All Datasets ---\n",
        "print(\"\\n--- [8/10] Combining all 3 datasets into one... ---\")\n",
        "combined_dataset = concatenate_datasets([\n",
        "    formatted_igbo_chat,\n",
        "    formatted_igbo_dict,\n",
        "    formatted_general_chat\n",
        "])\n",
        "final_dataset = combined_dataset.shuffle(seed=42)\n",
        "TOTAL_EXAMPLES = len(final_dataset)\n",
        "print(f\"--- ‚úÖ [8/10] TOTAL EXAMPLES FOR TRAINING: {TOTAL_EXAMPLES} ---\")\n",
        "\n",
        "# --- 9. Define Training Arguments ---\n",
        "print(\"\\n--- [9/10] Setting up Training Arguments... ---\")\n",
        "EFFECTIVE_BATCH_SIZE = 8 * 2\n",
        "global NEW_MAX_STEPS\n",
        "NEW_MAX_STEPS = TOTAL_EXAMPLES // EFFECTIVE_BATCH_SIZE\n",
        "print(f\"--- Total Examples: {TOTAL_EXAMPLES} ---\")\n",
        "print(f\"--- Effective Batch Size: {EFFECTIVE_BATCH_SIZE} ---\")\n",
        "print(f\"--- CALCULATED MAX STEPS: {NEW_MAX_STEPS} (for ~1 epoch) ---\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    max_steps = NEW_MAX_STEPS,\n",
        "    per_device_train_batch_size = 8,\n",
        "    gradient_accumulation_steps = 2,\n",
        "    optim = \"adamw_8bit\", # <-- FIX: Explicitly set 8-bit optimizer\n",
        "    learning_rate = 2e-5,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    save_strategy = \"steps\",\n",
        "    save_steps = 1000,\n",
        "    save_total_limit = 1,\n",
        "    push_to_hub = True,\n",
        "    hub_model_id = NEW_HUB_REPO,\n",
        "    hub_strategy = \"checkpoint\",\n",
        "    logging_steps = 500,\n",
        "    fp16 = True,\n",
        "    group_by_length = True,\n",
        "    report_to = \"none\",\n",
        ")\n",
        "print(f\"--- ‚úÖ [9/10] Training Arguments set for {NEW_MAX_STEPS} steps ---\")\n",
        "\n",
        "# --- 10. Initialize the Trainer ---\n",
        "print(\"\\n--- [10/10] Initializing the SFTTrainer... ---\")\n",
        "global trainer\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = final_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    args = training_args,\n",
        "    packing = True,\n",
        ")\n",
        "print(\"--- ‚úÖ [10/10] Trainer is ready! ---\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"   üöÄ ALL SETUP IS COMPLETE AND CORRECT üöÄ\")\n",
        "print(\"   You are ready to train the BILINGUAL CHATBOT.\")\n",
        "print(f\"   Your checkpoints will be saved to: {NEW_HUB_REPO}\")\n",
        "print(\"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "AieiId7-zCsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Start the \"Colab Relay Race\" (Worker 1 ONLY)\n",
        "print(f\"--- üöÄ STARTING the 'Colab Relay Race' (Worker 1) ---\")\n",
        "print(f\"--- This will run from step 0 and create the first checkpoint. ---\")\n",
        "print(f\"--- Training for a total of {NEW_MAX_STEPS} steps. ---\")\n",
        "\n",
        "start_time_train = time.time()\n",
        "try:\n",
        "    # Worker 1 just calls .train() to start from scratch\n",
        "    trainer.train()\n",
        "    print(\"\\n--- üéâ TRAINING COMPLETED NORMALLY! ---\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- üí• Training interrupted by unexpected error: {e} ---\")\n",
        "finally:\n",
        "    end_time_train = time.time()\n",
        "    print(f\"--- Training run duration: {(end_time_train - start_time_train) / 60:.2f} minutes ---\")\n",
        "    print(f\"--- üõë Session ended. Your checkpoint is safe on Hugging Face: {NEW_HUB_REPO} ---\")"
      ],
      "metadata": {
        "id": "F7V4QaiKzK7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Resume the \"Colab Relay Race\" (Worker 2, 3...)\n",
        "\n",
        "# --- Configuration ---\n",
        "HUB_MODEL_ID = NEW_HUB_REPO\n",
        "HUB_CHECKPOINT_SUBFOLDER = \"last-checkpoint\"\n",
        "LOCAL_CHECKPOINT_PATH = os.path.join(os.path.expanduser(\"~\"), \"local_hub_resume\")\n",
        "\n",
        "print(f\"--- üëü RESUMING Training (Worker 2/3/...) ---\")\n",
        "print(f\"--- ‚¨áÔ∏è Downloading latest checkpoint from Hub: {HUB_MODEL_ID} ---\")\n",
        "\n",
        "# --- 1. Download Checkpoint Files Locally ---\n",
        "try:\n",
        "    snapshot_download(\n",
        "        repo_id=HUB_MODEL_ID,\n",
        "        allow_patterns=[f\"{HUB_CHECKPOINT_SUBFOLDER}/*\"],\n",
        "        local_dir=LOCAL_CHECKPOINT_PATH,\n",
        "        local_dir_use_symlinks=False,\n",
        "        token=token # <-- This token IS defined from Cell 1.\n",
        "    )\n",
        "    print(\"--- ‚úÖ Checkpoint downloaded. ---\")\n",
        "except Exception as e:\n",
        "    print(f\"--- ‚ö†Ô∏è FAILED to download checkpoint. Did Worker 1 run and save a checkpoint? Error: {e} ---\")\n",
        "    raise e\n",
        "\n",
        "# --- 2. Define the Local Path to Resume From ---\n",
        "RESUME_PATH = os.path.join(LOCAL_CHECKPOINT_PATH, HUB_CHECKPOINT_SUBFOLDER)\n",
        "print(f\"--- üéØ Resuming from LOCAL PATH: {RESUME_PATH} ---\")\n",
        "\n",
        "# --- 3. Run the Training ---\n",
        "start_time_train = time.time()\n",
        "try:\n",
        "    trainer.train(resume_from_checkpoint = RESUME_PATH)\n",
        "    print(\"\\n--- üéâ TRAINING COMPLETED NORMALLY! ---\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- üí• Training interrupted by unexpected error: {e} ---\")\n",
        "finally:\n",
        "    end_time_train = time.time()\n",
        "    print(f\"--- Training run duration: {(end_time_train - start_time_train) / 60:.2f} minutes ---\")\n",
        "    print(f\"--- üõë Session ended. The latest checkpoint is safe on Hugging Face. ---\")"
      ],
      "metadata": {
        "id": "oQJYvEv3zNpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Merge and Save to GGUF\n",
        "print(\"--- Merging model and converting to GGUF format... ---\")\n",
        "\n",
        "# 1. Save 16-bit Merged Model (Good for other HF users)\n",
        "# This merges the 120MB adapter into the 2GB base model\n",
        "model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "model.push_to_hub_merged(f\"{YOUR_MODEL_REPO}-merged\", tokenizer, save_method = \"merged_16bit\", token=token)\n",
        "\n",
        "# 2. Save to GGUF (Good for Ollama / LM Studio)\n",
        "# This creates a 'q4_k_m' quantized version which is fast and small\n",
        "model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "model.push_to_hub_gguf(f\"{YOUR_MODEL_REPO}-GGUF\", tokenizer, quantization_method = \"q4_k_m\", token=token)\n",
        "\n",
        "print(f\"--- ‚úÖ Success! Your model is saved to {YOUR_MODEL_REPO}-GGUF ---\")\n",
        "print(\"You can now download the .gguf file and run it locally!\")"
      ],
      "metadata": {
        "id": "qzxiVaoEHWvb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}